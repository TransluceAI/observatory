{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Before running this notebook, make sure the notebook is using the `expgen` environment.\n",
    "You can follow the instructions [here](../../../README.md#using-jupyter-notebooks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "from util.subject import Subject, llama31_8B_instruct_config\n",
    "from activations.exemplars import ExemplarSplit, ExemplarType\n",
    "from activations.exemplars_wrapper import ExemplarConfig, ExemplarsWrapper\n",
    "from activations.dataset import fineweb_dset_config, ultrachat_dset_config\n",
    "from activations.exemplars_computation import (\n",
    "    compute_exemplars_for_layer,\n",
    ")\n",
    "from explanations.explanations import ActivationSign, NeuronExplanations, simulate_and_score\n",
    "from explanations.explanations_wrapper import ExplanationConfig, ExplanationsWrapper\n",
    "from explanations.simulation_utils import FinetunedSimulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate descriptions for a single neuron in Llama 3.1 8B Instruct.\n",
    "This notebook assumes access to at least 1 GPU that can fit a Llama-3.1-8B-Instruct model.\n",
    "\n",
    "It will download:\n",
    "- FineWeb and UltraChat 200k datasets.\n",
    "- Llama-3.1-8B-Instruct (15GB)\n",
    "- the fine-tuned explainer (15GB)\n",
    "- the fine-tuned simulator (15GB)\n",
    "\n",
    "Because there are 3 models involved in the process, if you only have 1 GPU, you will need to restart the session after each step is done. The progress at each step is cached so re-executing the cells is fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Pick a neuron in Llama-3.1-8B-Instruct.\n",
    "layer $\\in$ [0, 31], and neuron_idx $\\in$ [0, 14336]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some neurons to get you started:\n",
    "layer, neuron_idx = 5, 2183  # RL-related\n",
    "layer, neuron_idx = 5, 14249  # East/west coast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Generate exemplars that maximally and minimally activate the neuron\n",
    "We do this by going through random sequences and keeping in track of the top and bottom 100 sequences in terms of their neuron activation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject model which contains the neuron of interest.\n",
    "subject = Subject(llama31_8B_instruct_config)\n",
    "\n",
    "exemplar_config = ExemplarConfig(\n",
    "    hf_model_id=subject.lm_config.hf_model_id,\n",
    "    # The two datasets we are going to sample sequences from\n",
    "    hf_dataset_configs=(fineweb_dset_config, ultrachat_dset_config),\n",
    "    sampling_ratios=[0.9, 0.1],\n",
    "    # We are going to go through 20,000 sequences per polarity (+/-).\n",
    "    num_seqs=50_000,\n",
    "    # Each sequence will be at least 95 tokens long.\n",
    "    seq_len=95,\n",
    "    # We are going to keep the top and bottom 100 sequences per neuron.\n",
    "    k=100,\n",
    ")\n",
    "\n",
    "# This will download FineWeb and UltraChat 200k if they are not already present.\n",
    "# It will take a couple of minutes to download the datasets.\n",
    "exemplars_wrapper = ExemplarsWrapper(\n",
    "    data_dir=\"single_neuron_experiment/\",\n",
    "    config=exemplar_config,\n",
    "    subject=subject,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the exemplars for:\n",
    "- the train split (this will be used to generate descriptions)\n",
    "- the validation split (this will be used to score descriptions)\n",
    "\n",
    "This will take around 7 minutes per split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will download the subject model (Llama-3.1-8B-Instruct) if it is not already present.\n",
    "compute_exemplars_for_layer(\n",
    "    exemplars_wrapper=exemplars_wrapper, layer=layer, split=ExemplarSplit.TRAIN\n",
    ")\n",
    "compute_exemplars_for_layer(\n",
    "    exemplars_wrapper=exemplars_wrapper, layer=layer, split=ExemplarSplit.VALID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the exemplars to see if there are any interesting patterns in the activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplars_wrapper.visualize_neuron_exemplars(\n",
    "    layer=layer,\n",
    "    neuron_idx=neuron_idx,\n",
    "    exemplar_split=ExemplarSplit.TRAIN,\n",
    "    indices=list(range(10)),  # Change this to visualize specific ranks in [0, 100].\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Generate descriptions given the exemplars\n",
    "We use our fine-tuned Llama 3.1 8B Instruct explainer to generate descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_config = ExplanationConfig(\n",
    "    exemplar_config=exemplar_config,\n",
    "    # Include the top 20 exemplars in the prompt to the explainer.\n",
    "    exem_slice_for_exp=(0, 20, 1),\n",
    "    # Explainer model is on HuggingFace.\n",
    "    explainer_model_name=\"Transluce/llama_8b_explainer\",\n",
    "    examples_placement=\"no_examples\",\n",
    "    # We sample 50 explanations.\n",
    "    num_explanation_samples=50,\n",
    "    # Simulator model is also on HuggingFace. More on this later.\n",
    "    simulator_model_name=\"Transluce/llama_8b_simulator\",\n",
    ")\n",
    "explanations_wrapper = ExplanationsWrapper(\n",
    "    save_path=\"single_neuron_experiment/\",\n",
    "    config=explanation_config,\n",
    "    exemplars_data_dir=\"single_neuron_experiment/\",\n",
    "    subject=subject,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In order to do the description generation, you need to have an additional GPU available since the subject model is using the first GPU (when generating exemplars). If there is only 1 GPU, restart the session and re-execute the cells from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the explainer model.\n",
    "# This will download the finetuned explainer model if it is not already present.\n",
    "explanations_wrapper.initialize_explainer()\n",
    "\n",
    "# Generate 50 descriptions using our fine-tuned explainer for both max/minimally activating exemplars.\n",
    "explanations_wrapper.generate_explanations_for_neuron(layer, neuron_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the sampled descriptions. It's quite hard to tell which is the best since there are so many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = explanations_wrapper.get_explanations_for_neuron(\n",
    "    layer, neuron_idx, exem_splits=[ExemplarSplit.VALID]\n",
    ")\n",
    "for i, exp in enumerate(explanations[\"negative\"]):\n",
    "    print(f\"description {i + 1}: {exp[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Score the descriptions\n",
    "We can use our fine-tuned Llama 3.1 8B Instruct simulator to measure the quality of the 50 descriptions.\n",
    "\n",
    "Note: In order to do the scoring, you need to have an additional GPU available since the explainer model is using the first GPU (when generating descriptions). If there is only 1 GPU, restart the session and re-execute the cells from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will download the finetuned simulator model if it is not already present.\n",
    "simulator = FinetunedSimulator.setup(\n",
    "    model_path=\"Transluce/llama_8b_simulator\",\n",
    "    add_special_tokens=True,\n",
    "    gpu_idx=1,  # If there are multiple GPUs, set this to > 0.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_explanations = explanations_wrapper.get_neuron_scored_explanations(layer, neuron_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations: Dict[ActivationSign, List[str]] = neuron_explanations.explanations\n",
    "split_exemplars = explanations_wrapper.get_split_neuron_exemplars(\n",
    "    True, ExemplarSplit.VALID, layer, neuron_idx\n",
    ")\n",
    "\n",
    "results = {act_sign: [] for act_sign in ActivationSign}\n",
    "for act_sign, explanations_list in explanations.items():\n",
    "    extype = ExemplarType.MAX if act_sign == ActivationSign.POS else ExemplarType.MIN\n",
    "    results[act_sign] = simulate_and_score(\n",
    "        split_exemplars=split_exemplars,\n",
    "        explanations=explanations_list,\n",
    "        exemplar_type=extype,\n",
    "        simulator=simulator,\n",
    "    )\n",
    "scored_neuron_explanations = NeuronExplanations(\n",
    "    neuron_id=neuron_explanations.neuron_id,\n",
    "    explanations=results,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at only the top-scoring explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_explanations = scored_neuron_explanations.get_best_explanations(\n",
    "    exemplar_splits=[ExemplarSplit.VALID]\n",
    ")\n",
    "for act_sign, neuron_expl in best_explanations.items():\n",
    "    print(f\"Top explanation for {act_sign}: \")\n",
    "    explanation_str = neuron_expl.explanation\n",
    "    score = neuron_expl.get_preferred_score(exemplar_splits=[ExemplarSplit.VALID])\n",
    "    print(f\"score: {score:.2f}: {explanation_str}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
